{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KswTMHmlU0rb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import euclidean\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define Models and Criteria\n",
        "models = [\"SBERT\", \"USE\", \"GPT-Embedding\", \"BERTScore\", \"GloVe\"]\n",
        "criteria = [\"Cosine Similarity\", \"Inference Speed\", \"Memory Usage\", \"Model Size\", \"Accuracy\"]\n",
        "\n",
        "# Step 2:  Decision Matrix\n",
        "decision_matrix = np.array([\n",
        "    [0.92, 0.015, 700, 420, 0.89],  # SBERT\n",
        "    [0.87, 0.010, 550, 310, 0.85],  # USE\n",
        "    [0.94, 0.020, 1024, 500, 0.91], # GPT-Embedding\n",
        "    [0.89, 0.018, 800, 400, 0.88],  # BERTScore\n",
        "    [0.83, 0.012, 600, 280, 0.80]   # GloVe\n",
        "])"
      ],
      "metadata": {
        "id": "TSAS1EUtVU2T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Normalize the Matrix\n",
        "norm_matrix = decision_matrix / np.linalg.norm(decision_matrix, axis=0)\n",
        "\n",
        "# Step 4: Assign Weights (higher weight to Accuracy & Cosine Similarity)\n",
        "weights = np.array([0.3, 0.2, 0.1, 0.1, 0.3])\n",
        "weighted_matrix = norm_matrix * weights"
      ],
      "metadata": {
        "id": "_urYTVtSViHM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Identify Ideal Best and Worst\n",
        "ideal_best = np.max(weighted_matrix, axis=0)\n",
        "ideal_worst = np.min(weighted_matrix, axis=0)\n",
        "\n",
        "# Step 6: Compute Euclidean Distance from Best & Worst\n",
        "distance_best = np.array([euclidean(row, ideal_best) for row in weighted_matrix])\n",
        "distance_worst = np.array([euclidean(row, ideal_worst) for row in weighted_matrix])"
      ],
      "metadata": {
        "id": "8zJiymy8Vm0D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topsis_scores = distance_worst / (distance_best + distance_worst)"
      ],
      "metadata": {
        "id": "5qBN318DVqc1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topsis_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lE3_SZsAVwuE",
        "outputId": "0147c7e0-7d06-427c-d5f7-61cc855483b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.52119254, 0.13025492, 1.        , 0.69838035, 0.15932359])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rankings = np.argsort(-topsis_scores)  # Higher score is better\n",
        "ranked_models = [models[i] for i in rankings]\n",
        "\n",
        "# Step 9: Display Results\n",
        "results_df = pd.DataFrame({\n",
        "    \"Model\": models,\n",
        "    \"TOPSIS Score\": topsis_scores,\n",
        "    \"Rank\": rankings + 1\n",
        "})\n",
        "\n",
        "print(results_df.sort_values(by=\"Rank\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IoWTMohVyjq",
        "outputId": "ea09a76e-ffa6-4b43-e700-ee1f5bc21e57"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Model  TOPSIS Score  Rank\n",
            "2  GPT-Embedding      1.000000     1\n",
            "4          GloVe      0.159324     2\n",
            "0          SBERT      0.521193     3\n",
            "1            USE      0.130255     4\n",
            "3      BERTScore      0.698380     5\n"
          ]
        }
      ]
    }
  ]
}